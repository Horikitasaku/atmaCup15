{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model,AutoModel, AutoTokenizer\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "struct = 'lgbm'\n",
    "struct = 'tabnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend to: 1780\n",
      "need to predict: 1780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>0669cc0219d468761195</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>111adb8835b8a1a2cf54</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>1fc8683c393432a2f9c7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>2290175205d55e81b197</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>28f173b60331d5cabb0d</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_id              anime_id  score\n",
       "0  0008e10fb39e55447333  0669cc0219d468761195      2\n",
       "1  0008e10fb39e55447333  111adb8835b8a1a2cf54     10\n",
       "2  0008e10fb39e55447333  1fc8683c393432a2f9c7      1\n",
       "3  0008e10fb39e55447333  2290175205d55e81b197      8\n",
       "4  0008e10fb39e55447333  28f173b60331d5cabb0d      9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"./train/train.csv\")\n",
    "test_df = pd.read_csv(\"./train/test.csv\")\n",
    "anime_df = pd.read_csv(\"./train/anime.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inter = list(set(train_df.user_id.unique())&set(test_df.user_id.unique()))\n",
    "print('recommend to:',len(inter))\n",
    "test_outer = list(set(test_df.user_id.unique())-set(train_df.user_id.unique()))\n",
    "print('need to predict:',len(inter))\n",
    "# train_df = anime_info.merge(train_df,on='anime_id')\n",
    "# test_df = anime_info.merge(test_df,on='anime_id')\n",
    "\n",
    "do_embedding = True\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inter = train_df[train_df.user_id.isin(inter)]\n",
    "\n",
    "test_inter = test_df[test_df.user_id.isin(inter)]\n",
    "test_outer = test_df[test_df.user_id.isin(test_outer)]\n",
    "# test_outer = test_outer[['user_id','anime_id']].merge(train_scores,on=['anime_id'])\n",
    "# test_outer.to_csv('submission_outer.csv',index=False)\n",
    "# test_inter.to_csv('test/test_inter.csv',index=False)\n",
    "# test_outer.to_csv('test/test_outer.csv',index=False)\n",
    "# train_inter.to_csv('train/train_inter.csv',index=False)\n",
    "# train_outer.to_csv('train/train_outer.csv')\n",
    "# train_outer.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_mean(x):\n",
    "    counts = x.value_counts()\n",
    "    total = counts.sum()\n",
    "    weighted_sum = sum(score * count for score, count in counts.items())\n",
    "    return weighted_sum / total\n",
    "\n",
    "def to_minutes(s):\n",
    "\n",
    "    match = re.search(r'(\\d+) hr.*? (\\d+) min.', s)\n",
    "    if match:\n",
    "        return int(match.group(1)) * 60 + int(match.group(2))\n",
    "    match = re.search(r'(\\d+) min', s)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 1 # for Unknown\n",
    "\n",
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df, right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\",\"user_id\"])\n",
    "\n",
    "def merge_by_anime_id_with(left_df, right_df):\n",
    "    return pd.merge(left_df, right_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "\n",
    "def merge_by_user_id_with(left_df, right_df):\n",
    "    return pd.merge(left_df, right_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# train_scores = train_df.groupby('anime_id')['score'].apply(weighted_mean)\n",
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "        \"watching\",\"completed\",\"on_hold\",\"dropped\",\"plan_to_watch\"\n",
    "    ]\n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]\n",
    "    # return pd.merge(input_df, anime_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\",\"user_id\"])\n",
    "\n",
    "\n",
    "def create_anime_type_one_hot_encoding(input_df):\n",
    "    \n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        out_df[value] = is_value.astype(int)\n",
    "        \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, out_df)\n",
    "def type(input_df):\n",
    "    # 単純にラベルエンコーディング\n",
    "    encoder = LabelEncoder()\n",
    "    type_encoder = anime_df[['anime_id']].copy()\n",
    "    type_encoder[\"type_label\"] = encoder.fit_transform(anime_df[\"type\"])\n",
    "    return merge_by_anime_id(input_df, type_encoder)\n",
    "\n",
    "def create_anime_type_one_hot_encoding(input_df):\n",
    "    \n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        out_df[value] = is_value.astype(int)\n",
    "        \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, out_df)\n",
    "\n",
    "def create_anime_type_count_encoding(input_df):\n",
    "    count = anime_df[\"type\"].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"tyoe_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)\n",
    "\n",
    "def create_licensors_count_encoding(input_df):\n",
    "    count = anime_df[\"licensors\"].map(anime_df[\"licensors\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"licensors_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)\n",
    "\n",
    "def studios(input_df):\n",
    "    # 単純にラベルエンコーディング\n",
    "    encoder = LabelEncoder()\n",
    "    studios_encode = anime_df[['anime_id']].copy()\n",
    "    studios_encode[\"studios\"] = encoder.fit_transform(anime_df[\"studios\"])\n",
    "    return merge_by_anime_id(input_df, studios_encode)\n",
    "\n",
    "def create_studio_count_encoding(input_df):\n",
    "    count = anime_df[\"studios\"].map(anime_df[\"studios\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"studios_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)\n",
    "\n",
    "def create_source_count_encoding(input_df):\n",
    "    count = anime_df[\"source\"].map(anime_df[\"source\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"source_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)\n",
    "\n",
    "def source(input_df):\n",
    "    # 単純にラベルエンコーディング\n",
    "    encoder = LabelEncoder()\n",
    "    source_encode = anime_df[['anime_id']].copy()\n",
    "    source_encode[\"source\"] = encoder.fit_transform(anime_df[\"source\"])\n",
    "    return merge_by_anime_id(input_df, source_encode)\n",
    "\n",
    "def create_rating_count_encoding(input_df):\n",
    "    count = anime_df[\"rating\"].map(anime_df[\"rating\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"rating_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)\n",
    "\n",
    "def create_genres_onehot_encoding(input_df):\n",
    "    \"\"\"Create 26-dim embedding\"\"\"\n",
    "    chars = ['Comedy', 'Sci-Fi', 'Seinen', 'Slice of Life', 'Space',\n",
    "       'Adventure', 'Mystery', 'Historical', 'Supernatural', 'Fantasy',\n",
    "       'Ecchi', 'School', 'Harem', 'Romance', 'Shounen', 'Action',\n",
    "       'Magic', 'Sports', 'Super Power', 'Drama', 'Thriller', 'Music',\n",
    "       'Shoujo', 'Demons', 'Mecha', 'Game', 'Josei', 'Cars',\n",
    "       'Psychological', 'Parody', 'Samurai', 'Military', 'Shoujo Ai',\n",
    "       'Kids', 'Martial Arts', 'Horror', 'Dementia', 'Vampire',\n",
    "       'Shounen Ai', 'Hentai', 'Yaoi', 'Police']\n",
    "    genres = anime_df[['anime_id','genres']]\n",
    "    genres.loc[:,chars] = 0\n",
    "    genres['genres'] = genres['genres'].str.split(',')\n",
    "    # genres[chars] = 0\n",
    "    for i, row in tqdm(genres.iterrows(),total=2000):\n",
    "        for index in (s.strip() for s in row['genres']):\n",
    "                genres.loc[i,index] = 1\n",
    "    genres = genres.drop('genres',axis=1)\n",
    "    return merge_by_anime_id(input_df, genres)\n",
    "\n",
    "\n",
    "def create_producer_onehot_encoding(input_df):\n",
    "    \"\"\"Create 26-dim embedding\"\"\"\n",
    "    producer = anime_df['producers'].str.split(',')\n",
    "    all_producer = list(map(str.strip,producer.explode().unique()))\n",
    "    producer = anime_df[['anime_id']]\n",
    "    producer.loc[:,all_producer] = 0\n",
    "    producer['producers'] = anime_df['producers'].str.split(',')\n",
    "    # genres[chars] = 0\n",
    "    for i, row in tqdm(producer.iterrows(),total=2000):\n",
    "        for index in (s.strip() for s in row['producers']):\n",
    "                producer.loc[i,index] = 1\n",
    "    \n",
    "    return merge_by_anime_id(input_df, producer).drop('producers',axis=1)\n",
    "\n",
    "\n",
    "def create_anime_type_one_hot_encoding(input_df):\n",
    "    \n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        out_df[value] = is_value.astype(int)\n",
    "        \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, out_df)\n",
    "\n",
    "def create_duration2min(input_df):\n",
    "    time_min = anime_df[['anime_id']]\n",
    "    time_min['minutes'] = anime_df['duration'].apply(to_minutes)\n",
    "    return merge_by_anime_id(input_df, time_min)\n",
    "from datetime import datetime\n",
    "def to_year(s):\n",
    "    match = re.search(r'\\d{4}', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 1000\n",
    "    \n",
    "def year_pre(input_df):\n",
    "    year = anime_df[['anime_id']]\n",
    "    encoder = LabelEncoder()\n",
    "    year['year'] = encoder.fit_transform(anime_df['aired'].apply(to_year))\n",
    "    \n",
    "    return merge_by_anime_id(input_df, year)\n",
    "\n",
    "def merge_embedding(input_df):\n",
    "    embeds = np.load(\"train/anime_jpbert_embeddings.npy\")\n",
    "    ids = np.load(\"train/anime_jpbert_ids.npy\")\n",
    "    embeds_list = []\n",
    "    for l in range(embeds.shape[0]):\n",
    "        embeds_list.append(embeds[l,:])\n",
    "    anime_embed = pd.DataFrame(data={\"anime_id\": ids, \"embed\" : embeds_list})\n",
    "    embed_array = np.array(anime_embed[\"embed\"].tolist())\n",
    "\n",
    "    for i in range(768):\n",
    "        anime_embed[f\"embed_{i}\"] = embed_array[:,i]\n",
    "\n",
    "    anime_embed.drop(\"embed\", axis=1, inplace=True)\n",
    "    return merge_by_anime_id(input_df, anime_embed)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pickle\n",
    "from gensim.models import word2vec,Word2Vec\n",
    "\n",
    "def create_tfidf_matrix(input_df):\n",
    "    text_columns = ['genres', 'japanese_name', 'type', 'episodes', 'aired', 'producers', 'licensors', 'studios', 'source', 'duration', 'rating']\n",
    "\n",
    "    tf = TfidfVectorizer()\n",
    "    try:\n",
    "        tf = pickle.load(open(\"vectorizer.pickle\", \"rb\"))\n",
    "        print('loaded tf')\n",
    "    except:\n",
    "        tf.fit(anime_df[text_columns].astype(str).apply(lambda x: ';'.join(x), axis=1).tolist()) \n",
    "        pickle.dump(tf, open(\"vectorizer.pickle\", \"wb\"))\n",
    "    tfidf_matrix = tf.fit_transform(anime_df[text_columns].astype(str).apply(lambda x: ';'.join(x), axis=1).tolist())\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix) \n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_df['anime_id'],columns=anime_df['anime_id'])\n",
    "    svd = TruncatedSVD(n_components=10,random_state=42)\n",
    "\n",
    "    svd_arr = svd.fit_transform(cosine_sim_df)\n",
    "    col_df = pd.DataFrame(\n",
    "    svd_arr,\n",
    "    index=cosine_sim_df.index,\n",
    "    columns=[f\"svd_{ix}\" for ix in range(10)],\n",
    "    ).reindex()\n",
    "    return merge_by_anime_id_with(input_df, col_df)\n",
    "\n",
    "def create_user_anime_vector(input_df):\n",
    "    df = pd.concat([train_df,test_df])\n",
    "    if 'score' in input_df.columns:\n",
    "        user_matrix = create_tfidf_matrix(df).drop(['anime_id','score'],axis=1).groupby('user_id').mean().reindex()\n",
    "    else:\n",
    "        user_matrix = create_tfidf_matrix(df).drop(['anime_id'],axis=1).groupby('user_id').mean().reindex()\n",
    "    \n",
    "    return merge_by_user_id_with(input_df, user_matrix).drop('user_id',axis=1)\n",
    "\n",
    "def create_01EDA_user_vector(input_df):\n",
    "    df = pd.concat([train_df,test_df])\n",
    "    if 'score' in input_df.columns:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_01EDA',mode='user').drop(['anime_id','score'],axis=1).groupby('user_id').mean().reindex()\n",
    "    else:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_01EDA',mode='user').drop(['anime_id'],axis=1).groupby('user_id').mean().reindex()\n",
    "    \n",
    "    return merge_by_user_id_with(input_df, user_matrix).drop('user_id',axis=1)\n",
    "\n",
    "def create_large_user_vector(input_df):\n",
    "    df = pd.concat([train_df,test_df])\n",
    "    if 'score' in input_df.columns:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_large',mode='user').drop(['anime_id','score'],axis=1).groupby('user_id').mean().reindex()\n",
    "    else:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_large',mode='user').drop(['anime_id'],axis=1).groupby('user_id').mean().reindex()\n",
    "    \n",
    "    return merge_by_user_id_with(input_df, user_matrix).drop('user_id',axis=1)\n",
    "\n",
    "def create_small_user_vector(input_df):\n",
    "    df = pd.concat([train_df,test_df])\n",
    "    if 'score' in input_df.columns:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_small',mode='user').drop(['anime_id','score'],axis=1).groupby('user_id').mean().reindex()\n",
    "    else:\n",
    "        user_matrix = create_llma_anime_vector(df,embed_source='mBERT_embedding_small',mode='user').drop(['anime_id'],axis=1).groupby('user_id').mean().reindex()\n",
    "    \n",
    "    return merge_by_user_id_with(input_df, user_matrix).drop('user_id',axis=1)\n",
    "\n",
    "def create_llma_anime_vector(input_df,embed_source,mode='anime'):\n",
    "    embeddings = np.load(f\"./train/{embed_source}.npy\")\n",
    "    df_anime_preprocessd = anime_df[['anime_id']].copy()\n",
    "    # cosine_sim = cosine_similarity(embeddings)\n",
    "    # svd = TruncatedSVD(n_components=20,random_state=42)\n",
    "    # svd_arr = svd.fit_transform(cosine_sim)\n",
    "    \n",
    "    # df_anime_preprocessd = df_anime_preprocessd.drop(columns=[\"combined_features\"])\n",
    "    embeddings_columns = [f\"{embed_source}_{mode}_{i}\" for i in range(embeddings.shape[1])]\n",
    "    embeddings_df = pd.DataFrame(data=embeddings, columns=embeddings_columns)\n",
    "    df_anime_preprocessd = df_anime_preprocessd.join(embeddings_df)\n",
    "    \n",
    "    return merge_by_anime_id_with(input_df, df_anime_preprocessd)\n",
    "\n",
    "def create_01EDA_anime_vector(input_df):\n",
    "    return create_llma_anime_vector(input_df,embed_source='mBERT_embedding_01EDA').drop(columns=[\"anime_id\",\"user_id\"])\n",
    "def create_large_anime_vector(input_df):\n",
    "    return create_llma_anime_vector(input_df,embed_source='mBERT_embedding_large').drop(columns=[\"anime_id\",\"user_id\"])\n",
    "def create_small_anime_vector(input_df):\n",
    "    return create_llma_anime_vector(input_df,embed_source='mBERT_embedding_small').drop(columns=[\"anime_id\",\"user_id\"])\n",
    "\n",
    "def create_w2v_anime_vector(input_df,mode='anime'):\n",
    "    text_columns = ['genres', 'japanese_name', 'type', 'episodes', 'aired', 'producers', 'licensors', 'studios', 'source']\n",
    "    text_data = anime_df[text_columns].astype(str).apply(lambda x: ';;'.join(x), axis=1).tolist()\n",
    "\n",
    "    text1 = [x.strip() for _ in text_data for x in re.split(';;',_) ] # テキストデータをコンマやセミコロンで分割して、空白を除去する\n",
    "    text_data.append(text1)\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in text_data] \n",
    "    train_sentence_list = text_data + shuffled_sentence_list\n",
    "    vector_size = 128\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": 42,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    try:\n",
    "        model = Word2Vec.load('model/w2v.model')\n",
    "    except:\n",
    "        model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "        model.save('model/w2v.model')\n",
    "    vects = {}\n",
    "    for i, row in anime_df.iterrows():\n",
    "        vect = []\n",
    "        for col in text_columns:\n",
    "            vect.append(model.wv[row[col]])\n",
    "        vects[row.anime_id] = np.concatenate(vect,axis=0)\n",
    "    wv_matirx = pd.DataFrame(vects).T\n",
    "    # svd = TruncatedSVD(n_components=20,random_state=42)\n",
    "\n",
    "    # svd_cos = svd.fit_transform(cosine_similarity(wv_matirx))\n",
    "    wv_cos = pd.DataFrame(wv_matirx,\n",
    "                          index=wv_matirx.index.values.tolist(),\n",
    "                          columns=[f\"wv_{mode}_cos_vec_{i}\" for i in range(wv_matirx.shape[1])]\n",
    "                          )\n",
    "    wv_cos.index.name = 'anime_id'\n",
    "    return merge_by_anime_id_with(input_df,wv_cos.reindex())\n",
    "\n",
    "def create_w2v_user_vector(input_df):\n",
    "    df = pd.concat([train_df,test_df])\n",
    "    if 'score' in input_df.columns:\n",
    "        user_matrix = create_w2v_anime_vector(df,mode='user').drop(['anime_id','score'],axis=1).groupby('user_id').mean().reindex()\n",
    "    else:\n",
    "        user_matrix = create_w2v_anime_vector(df,mode='user').drop(['anime_id'],axis=1).groupby('user_id').mean().reindex()\n",
    "    \n",
    "    return merge_by_user_id_with(input_df, user_matrix).drop('user_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_feature(input_df):\n",
    "    \n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_user_anime_vector,\n",
    "        create_tfidf_matrix,\n",
    "        create_w2v_anime_vector,\n",
    "\n",
    "        create_w2v_user_vector,\n",
    "        create_01EDA_user_vector,\n",
    "        create_large_user_vector,\n",
    "        create_small_user_vector,\n",
    "\n",
    "        create_anime_numeric_feature,\n",
    "        \n",
    "        type,\n",
    "        studios,\n",
    "        source,\n",
    "        # create_anime_type_count_encoding,\n",
    "        \n",
    "        # create_anime_type_one_hot_encoding,\n",
    "        create_genres_onehot_encoding,\n",
    "        # create_producer_onehot_encoding,\n",
    "        # create_licensors_count_encoding,\n",
    "    \n",
    "        # create_studio_count_encoding,\n",
    "        # create_source_count_encoding,\n",
    "        create_rating_count_encoding,\n",
    "        create_duration2min,\n",
    "\n",
    "        create_01EDA_anime_vector,\n",
    "        create_large_anime_vector,\n",
    "        create_small_anime_vector,\n",
    "        \n",
    "        year_pre,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "\n",
    "        _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df['score'] = 0\n",
    "# train_test_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "# train_test_df = add_w2v_features_without_score(train_test_df)\n",
    "# train_df = train_test_df[train_test_df['score'] >= 0].copy().reset_index(drop=True)\n",
    "# test_df = train_test_df[train_test_df['score'] == 0].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df, test_df = add_w2v_features(train_df, test_df)\n",
    "if struct == 'lgbm':\n",
    "    train_feat_df = create_feature(train_df)\n",
    "    test_feat_df = create_feature(test_df)\n",
    "    for text_col in ['anime_id','score','user_id']:\n",
    "        if text_col in train_feat_df.columns:\n",
    "            train_feat_df = train_feat_df.drop(columns=text_col)\n",
    "        if text_col in test_feat_df.columns:\n",
    "            test_feat_df = test_feat_df.drop(columns=text_col)\n",
    "    X = train_feat_df.values\n",
    "    y = train_df[\"score\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_feat_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class TabNetBaseline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tabnet_params,\n",
    "        embedding_cols,\n",
    "        embedding_idx,\n",
    "        cat_dims,\n",
    "        embedding_dims,\n",
    "        splitter=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        seed=777\n",
    "        ):\n",
    "        self.tabnet_params = tabnet_params\n",
    "        self.embedding_cols = embedding_cols\n",
    "        self.embedding_idx = embedding_idx\n",
    "        self.cat_dims = cat_dims\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.splitter = splitter\n",
    "        self.seed = seed\n",
    "        self.models = []\n",
    "        self.oof_preds = None\n",
    "        self.pretrained_model = None\n",
    "\n",
    "    def prepare_pretrain_data(self, df_pretrain):\n",
    "\n",
    "        for embc, d in zip(self.embedding_cols, self.cat_dims):\n",
    "            df_pretrain[embc] = df_pretrain[embc].replace({-1: df_pretrain[embc].max() + 1})\n",
    "            assert (df_pretrain[embc].max()+1 == d)\n",
    "            assert (len(df_pretrain[embc].unique()) == d)\n",
    "            assert df_pretrain[embc].min()==0\n",
    "\n",
    "        return df_pretrain\n",
    "\n",
    "    def pretrain(self, df_pretrain):\n",
    "        if not self.pretrained_model:\n",
    "            df_pretrain = self.prepare_pretrain_data(df_pretrain)\n",
    "            train_unsp, val_unsup = train_test_split(df_pretrain,  test_size=0.3, random_state=self.seed)\n",
    "\n",
    "            unsupervised_model = TabNetPretrainer(\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=0.1),\n",
    "                scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "                cat_idxs=self.embedding_idx, \n",
    "                cat_dims=self.cat_dims,\n",
    "                cat_emb_dim=self.embedding_dims,\n",
    "                **self.tabnet_params\n",
    "            )\n",
    "\n",
    "            unsupervised_model.fit(\n",
    "                X_train=train_unsp.values,\n",
    "                eval_set=[val_unsup.values],\n",
    "                pretraining_ratio=0.8,\n",
    "                max_epochs=300\n",
    "            )\n",
    "\n",
    "            self.pretrained_model = unsupervised_model\n",
    "        return self.pretrained_model\n",
    "\n",
    "    def train(self, X, y, groups=None):\n",
    "        self.models = []\n",
    "        self.oof_preds = np.zeros(len(X))\n",
    "        scores = []\n",
    "        for train_index, valid_index in self.splitter.split(X, y, groups=groups):\n",
    "            unsupervised_model = self.pretrained_model\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "            reg = TabNetRegressor(\n",
    "                optimizer_fn=torch.optim.Adam,\n",
    "                optimizer_params=dict(lr=0.03),\n",
    "                scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                cat_idxs=self.embedding_idx, \n",
    "                cat_dims=self.cat_dims,\n",
    "                cat_emb_dim=self.embedding_dims,\n",
    "                **self.tabnet_params\n",
    "            )\n",
    "\n",
    "            reg.fit(\n",
    "                X_train=X_train.values, y_train=y_train.values.reshape(-1, 1),\n",
    "                eval_set=[(X_train.values, y_train.values.reshape(-1, 1)), (X_valid.values, y_valid.values.reshape(-1, 1))],\n",
    "                eval_name=['train', 'valid'],\n",
    "                eval_metric=['rmse'],\n",
    "                batch_size=2048, virtual_batch_size=2048,\n",
    "                drop_last=True,\n",
    "                from_unsupervised=unsupervised_model,\n",
    "                max_epochs=1000,\n",
    "                patience=12,\n",
    "                num_workers=4,\n",
    "            )\n",
    "            self.models.append(reg)\n",
    "\n",
    "            y_pred = reg.predict(X_valid.values)[:, 0]\n",
    "            self.oof_preds[valid_index] = y_pred\n",
    "            score = mean_squared_error(y_valid, y_pred, squared=False)  # RMSE score\n",
    "            scores.append(score)\n",
    "        self.cv_score = np.mean(scores)\n",
    "\n",
    "    def inference(self, X):\n",
    "        y_preds = []\n",
    "        for model in self.models:\n",
    "            y_pred = model.predict(X)[:, 0]\n",
    "            y_preds.append(y_pred)\n",
    "        y_preds = np.mean(y_preds, axis=0)\n",
    "        return y_preds\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        df_features_list = []\n",
    "        for model in self.models:\n",
    "            df = pd.DataFrame(data ={\n",
    "                \"feature_importance\" : model.feature_importances_,\n",
    "                \"feature_names\" : self.features\n",
    "            })\n",
    "            df_features_list.append(df)\n",
    "\n",
    "        df_features = pd.concat(df_features_list).sort_values(by='feature_importance', ascending=False)\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(5, 10))\n",
    "        sns.barplot(\n",
    "            data = df_features,\n",
    "            x = 'feature_importance',\n",
    "            y = 'feature_names',\n",
    "            capsize=0.1, errwidth=1.2,\n",
    "            ax = ax\n",
    "        )\n",
    "        return f, ax\n",
    "# Pretrainerのハイパーパラメータ\n",
    "tabnet_params = {\n",
    "    'mask_type' : 'entmax',\n",
    "    'n_d' : 64,\n",
    "    'n_a': 64,\n",
    "    'n_steps': 3,\n",
    "    'gamma': 0.9,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_train_test(train, test):\n",
    "    # train[\"train_test\"] = \"train\"\n",
    "    # test[\"train_test\"] = \"test\"\n",
    "    df_all = pd.concat([train, test])\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tf\n",
      "loaded tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Horikita_Saku\\AppData\\Local\\Temp\\ipykernel_26396\\934449817.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genres.loc[:,chars] = 0\n",
      "C:\\Users\\Horikita_Saku\\AppData\\Local\\Temp\\ipykernel_26396\\934449817.py:145: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  genres.loc[:,chars] = 0\n",
      "C:\\Users\\Horikita_Saku\\AppData\\Local\\Temp\\ipykernel_26396\\934449817.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  genres['genres'] = genres['genres'].str.split(',')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67e1e3f82aa4b0fa9eb80a1c3fd3422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Horikita_Saku\\AppData\\Local\\Temp\\ipykernel_26396\\934449817.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  time_min['minutes'] = anime_df['duration'].apply(to_minutes)\n",
      "C:\\Users\\Horikita_Saku\\AppData\\Local\\Temp\\ipykernel_26396\\934449817.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  year['year'] = encoder.fit_transform(anime_df['aired'].apply(to_year))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\kaggle\\atmacup\\#15\\TabNet.ipynb Cell 13\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m struct \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtabnet\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_feat_df \u001b[39m=\u001b[39m create_feature(train_df)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     test_feat_df \u001b[39m=\u001b[39m create_feature(test_df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# df_pretrain = create_feature(concat_train_test(train_df,test_df))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m text_col \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39manime_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[1;32md:\\Code\\kaggle\\atmacup\\#15\\TabNet.ipynb Cell 13\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m functions:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     func_name \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     _df \u001b[39m=\u001b[39m func(input_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     out_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([out_df, _df], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out_df\n",
      "\u001b[1;32md:\\Code\\kaggle\\atmacup\\#15\\TabNet.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m     user_matrix \u001b[39m=\u001b[39m create_tfidf_matrix(df)\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39manime_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mreindex()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m     user_matrix \u001b[39m=\u001b[39m create_tfidf_matrix(df)\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39manime_id\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mreindex()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m \u001b[39mreturn\u001b[39;00m merge_by_user_id_with(input_df, user_matrix)\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32md:\\Code\\kaggle\\atmacup\\#15\\TabNet.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=235'>236</a>\u001b[0m     tf\u001b[39m.\u001b[39mfit(anime_df[text_columns]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()) \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=236'>237</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(tf, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvectorizer.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfit_transform(anime_df[text_columns]\u001b[39m.\u001b[39;49mastype(\u001b[39mstr\u001b[39;49m)\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m'\u001b[39;49m\u001b[39m;\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(x), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m cosine_sim \u001b[39m=\u001b[39m cosine_similarity(tfidf_matrix) \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/kaggle/atmacup/%2315/TabNet.ipynb#X15sZmlsZQ%3D%3D?line=239'>240</a>\u001b[0m cosine_sim_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(cosine_sim, index\u001b[39m=\u001b[39manime_df[\u001b[39m'\u001b[39m\u001b[39manime_id\u001b[39m\u001b[39m'\u001b[39m],columns\u001b[39m=\u001b[39manime_df[\u001b[39m'\u001b[39m\u001b[39manime_id\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\apply.py:905\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    902\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[0;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\apply.py:1038\u001b[0m, in \u001b[0;36mFrameColumnApply.series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[39mfor\u001b[39;00m (arr, name) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex):\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# GH#35462 re-pin mgr in case setitem changed it\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     ser\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m mgr\n\u001b[1;32m-> 1038\u001b[0m     mgr\u001b[39m.\u001b[39;49mset_values(arr)\n\u001b[0;32m   1039\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(ser, \u001b[39m\"\u001b[39m\u001b[39m_name\u001b[39m\u001b[39m\"\u001b[39m, name)\n\u001b[0;32m   1040\u001b[0m     \u001b[39myield\u001b[39;00m ser\n",
      "File \u001b[1;32me:\\Envs\\kaggle1\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2137\u001b[0m, in \u001b[0;36mSingleBlockManager.set_values\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[39m# TODO(CoW) do we need to handle copy on write here? Currently this is\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39m# only used for FrameColumnApply.series_generator (what if apply is\u001b[39;00m\n\u001b[0;32m   2135\u001b[0m \u001b[39m# mutating inplace?)\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m values\n\u001b[1;32m-> 2137\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39m_mgr_locs \u001b[39m=\u001b[39m BlockPlacement(\u001b[39mslice\u001b[39m(\u001b[39mlen\u001b[39m(values)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if struct == 'tabnet':\n",
    "    train_feat_df = create_feature(train_df)\n",
    "    test_feat_df = create_feature(test_df)\n",
    "    \n",
    "    # df_pretrain = create_feature(concat_train_test(train_df,test_df))\n",
    "    for text_col in ['anime_id','score','user_id']:\n",
    "        if text_col in train_feat_df.columns:\n",
    "            train_feat_df = train_feat_df.drop(columns=text_col)\n",
    "        if text_col in test_feat_df.columns:\n",
    "            test_feat_df = test_feat_df.drop(columns=text_col)\n",
    "    df_pretrain = concat_train_test(train_feat_df,test_feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tf\n"
     ]
    }
   ],
   "source": [
    "text_columns = ['genres', 'japanese_name', 'type', 'episodes', 'aired', 'producers', 'licensors', 'studios', 'source', 'duration', 'rating']\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "try:\n",
    "    tf = pickle.load(open(\"vectorizer.pickle\", \"rb\"))\n",
    "    print('loaded tf')\n",
    "except:\n",
    "    tf.fit(anime_df[text_columns].astype(str).apply(lambda x: ';'.join(x), axis=1).tolist()) \n",
    "    pickle.dump(tf, open(\"vectorizer.pickle\", \"wb\"))\n",
    "tfidf_matrix = tf.fit_transform(anime_df[text_columns].astype(str).apply(lambda x: ';'.join(x), axis=1).tolist())\n",
    "# cosine_sim = cosine_similarity(tfidf_matrix) \n",
    "# cosine_sim_df = pd.DataFrame(tfidf_matrix, index=anime_df['anime_id'],columns=anime_df['anime_id'])\n",
    "# svd = TruncatedSVD(n_components=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pretrain[[\n",
    "        'type_label',\n",
    "        # 'studios',\n",
    "        'source',\n",
    "        # 'year'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embeddingするカラム\n",
    "embeding_cols = [\n",
    "        'type_label',\n",
    "        # 'studios',\n",
    "        'source',\n",
    "        # 'year'\n",
    "    ]\n",
    "# 各々のユニーク数\n",
    "col_uniques = [ len(df_pretrain[c].unique()) for c in  embeding_cols ]\n",
    "# カラムの番号\n",
    "embeding_idx = [ i for i, c in  enumerate(df_pretrain.columns) if c in embeding_cols]\n",
    "cat_dims = [ v for k, v in zip(embeding_cols, col_uniques) if k in embeding_cols]\n",
    "# 埋め込み次元\n",
    "embeding_dims = [ i // 2 if i<100 else 64 for i in cat_dims ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tabnet_mbert = TabNetBaseline(\n",
    "    tabnet_params=tabnet_params,\n",
    "    embedding_cols=embeding_cols,\n",
    "    embedding_idx=embeding_idx,\n",
    "    cat_dims=cat_dims,\n",
    "    embedding_dims=embeding_dims\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "tabnet_mbert.pretrain(df_pretrain)\n",
    "tabnet_mbert.pretrained_model.save_model('model/pretrain_mBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_x = df_merge_all.loc[df_merge_all[\"train_test\"]==\"train\"].copy().drop(columns=drop_cols)\n",
    "# train_y = df_merge_all.loc[df_merge_all[\"train_test\"]==\"train\"].copy()['score']\n",
    "# X = train_feat_df.values\n",
    "# y = train_df[\"score\"].values\n",
    "\n",
    "loaded_pretrain = TabNetPretrainer()\n",
    "loaded_pretrain.load_model('.pretrain_mBERT.zip')\n",
    "tabnet_mbert.pretrained_model = loaded_pretrain\n",
    "train_y = train_df.copy()['score'].astype(float)\n",
    "tabnet_mbert.train(train_feat_df.astype(float),train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv = fold.split(X, y)\n",
    "cv = list(cv) # split の返り値は generator なので list 化して何度も iterate できるようにしておく\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"mean_squared_error の root (0.5乗)\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred) ** .5\n",
    "import lightgbm as lgbm\n",
    "import xgboost\n",
    "def fit_lgbm(X, \n",
    "             y, \n",
    "             cv, \n",
    "             params: dict=None, \n",
    "             verbose: int=50):\n",
    "    \"\"\"lightGBM を CrossValidation の枠組みで学習を行なう function\"\"\"\n",
    "\n",
    "    # パラメータがないときは、空の dict で置き換える\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    n_records = len(X)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(tqdm(cv)): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "\n",
    "        clf = lgbm.LGBMRegressor(**params,early_stopping_rounds=500)\n",
    "\n",
    "\n",
    "            \n",
    "        # cv 内で train に定義された x_train で学習する\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_valid, y_valid)],  \n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose)\n",
    "\n",
    "        # cv 内で validation data とされた x_valid で予測をして oof_pred に保存していく\n",
    "        # oof_pred は全部学習に使わなかったデータの予測結果になる → モデルの予測性能を見る指標として利用できる\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        score = root_mean_squared_error(y_valid, pred_i)\n",
    "        print(f\" - fold{i + 1} - {score:.4f}\")\n",
    "\n",
    "    score = root_mean_squared_error(y, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISHI: Whole Score: {score:.4f}\")\n",
    "    return oof_pred, models\n",
    "params = {\n",
    "    # 目的関数. これの意味で最小となるようなパラメータを探します. \n",
    "    \"objective\": \"rmse\", \n",
    "\n",
    "    # 木の最大数. early_stopping という枠組みで木の数は制御されるようにしていますのでとても大きい値を指定しておきます.\n",
    "    \"n_estimators\": 20000, \n",
    "    \n",
    "     # 学習率. 小さいほどなめらかな決定境界が作られて性能向上に繋がる場合が多いです、\n",
    "    # がそれだけ木を作るため学習に時間がかかります\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"reg_alpha\":1e-5,\n",
    "    \"reg_lambda\":1e-5,\n",
    "\n",
    "    # 特徴重要度計算のロジック(後述)\n",
    "    \"importance_type\": \"gain\", \n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof, models = fit_lgbm(X, y=y, params=params, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_true=y, y_pred=oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_importance(models, feat_train_df):\n",
    "    \"\"\"lightGBM の model 配列の feature importance を plot する\n",
    "    CVごとのブレを boxen plot として表現します.\n",
    "\n",
    "    args:\n",
    "        models:\n",
    "            List of lightGBM models\n",
    "        feat_train_df:\n",
    "            学習時に使った DataFrame\n",
    "    \"\"\"\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[\"feature_importance\"] = model.feature_importances_\n",
    "        _df[\"column\"] = feat_train_df.columns\n",
    "        _df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], \n",
    "                                          axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby(\"column\")\\\n",
    "        .sum()[[\"feature_importance\"]]\\\n",
    "        .sort_values(\"feature_importance\", ascending=False).index[:50]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(order) * .25)))\n",
    "    sns.boxenplot(data=feature_importance_df, \n",
    "                  x=\"feature_importance\", \n",
    "                  y=\"column\", \n",
    "                  order=order, \n",
    "                  ax=ax, \n",
    "                  palette=\"viridis\", \n",
    "                  orient=\"h\")\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(\"Importance\")\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "fig, ax = visualize_importance(models, train_feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test_feat_df.values) for model in tqdm(models)])\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0) # axis=0 なので shape の `k` が潰れる "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.DataFrame({\n",
    "    \"score\": pred\n",
    "})\n",
    "score['score'] = score['score'].apply(lambda x: np.clip(x, 1, 10))\n",
    "score.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['score'] = pred\n",
    "test_df['score'] = test_df['score'].apply(lambda x: np.clip(x, 1, 10))\n",
    "test_df.to_csv(\"submission_outer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('./model')\n",
    "model = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "text_columns = ['genres', 'japanese_name', 'type', 'episodes', 'aired', 'producers', 'licensors', 'studios', 'source', 'duration', 'rating', 'members', 'watching', 'completed', 'on_hold', 'dropped', 'plan_to_watch']\n",
    "\n",
    "# text_columns = ['japanese_name']\n",
    "def get_gpt2_embeds(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs[0][0][0]\n",
    "    return embeddings.detach().cpu().numpy()\n",
    "def concat_with_column_names(row):\n",
    "    return ' '.join([f'{row[col]}' for col in text_columns])\n",
    "    return ' '.join([f'{col}: {row[col]}' for col in text_columns])\n",
    "if do_embedding == True:\n",
    "    ids_list = []\n",
    "    embeddings = []\n",
    "    for i, row in tqdm(anime_df.iterrows(),total=2000):\n",
    "        ids_list.append(row['anime_id'])\n",
    "        text = concat_with_column_names(row[text_columns])\n",
    "        embedding = get_gpt2_embeds(text)\n",
    "        embeddings.append(embedding)\n",
    "    np.save('train/anime_jpbert_ids.npy',np.array(ids_list))\n",
    "    np.save('train/anime_jpbert_embeddings.npy',np.array(embeddings))\n",
    "    print(len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inter_sub = pd.read_csv('submission_inner.csv')\n",
    "outer_sub = pd.read_csv('submission_outer.csv')\n",
    "all_sub = pd.concat([inter_sub,outer_sub])\n",
    "sub = pd.read_csv('test/test.csv')\n",
    "submission = sub.merge(all_sub,on=['user_id','anime_id'],how='left')\n",
    "submission['score'] = submission['score'].apply(lambda x: np.clip(x, 1, 10))\n",
    "submission['score'].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(sub.anime_id.unique())-set(all_sub.anime_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('submission.csv').score.to_csv('sub.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
